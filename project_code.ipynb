{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ic7Qe-QhYPp-"
      },
      "outputs": [],
      "source": [
        "! cp -r '/content/drive/MyDrive/CS 421/datasets' '/content/dataset_folders'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "! pip install transformers datasets sentencepiece protobuf==3.20.* tensorboardX\n",
        "! pip install accelerate -U"
      ],
      "metadata": {
        "id": "DLYDJj_hhhnA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "Zxu6eWGb_kkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Utils"
      ],
      "metadata": {
        "id": "SyDS2C1tgtRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import re\n",
        "import json\n",
        "import numpy as np\n",
        "import accelerate\n",
        "\n",
        "from datasets import Dataset, DatasetDict, load_dataset\n",
        "\n",
        "DATASET_ROOT = '/content/dataset_folders'\n",
        "\n",
        "\n",
        "class DatasetLoader(object):\n",
        "    def __init__(self, dataset_name, source_dataset_name, dataset_version, has_valid, split_map,\n",
        "                 batch_size, train_batch_idxs, test_batch_idxs, valid_batch_idxs=None):\n",
        "        self.data_root = DATASET_ROOT\n",
        "        self.dataset_name = dataset_name\n",
        "        self.source_dataset_name = source_dataset_name\n",
        "        self.dataset_version = dataset_version\n",
        "        self.has_valid = has_valid\n",
        "        self.split_map = split_map\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.train_batch_idxs = train_batch_idxs\n",
        "        self.test_batch_idxs = test_batch_idxs\n",
        "        self.valid_batch_idxs = valid_batch_idxs\n",
        "\n",
        "        assert self.split_map is not None\n",
        "\n",
        "\n",
        "    def load_from_source(self):\n",
        "        if self.source_dataset_name is None:\n",
        "            self.source_dataset_name = self.dataset_name\n",
        "        if self.dataset_version is None:\n",
        "            datasets = load_dataset(self.source_dataset_name)\n",
        "        else:\n",
        "            datasets = load_dataset(self.source_dataset_name, self.dataset_version)\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def to_json(self, datasets):\n",
        "        for k, v in self.split_map.items():\n",
        "            datasets[v].to_json(f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_{k}.json')\n",
        "\n",
        "\n",
        "    def load_from_json(self):\n",
        "        data_files = {\n",
        "            'train': f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_train.json',\n",
        "            'test': f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_test.json',\n",
        "        }\n",
        "\n",
        "        if self.has_valid:\n",
        "            data_files.update({'valid': f'{self.data_root}/{self.dataset_name}/{self.dataset_name}_valid.json',})\n",
        "\n",
        "        datasets = load_dataset('json', data_files=data_files)\n",
        "        datasets = self._post_process(datasets)\n",
        "\n",
        "        # subsample training dataset if needed\n",
        "        num_train = len(datasets['train'])\n",
        "        idxs = list()\n",
        "        for idx in self.train_batch_idxs:\n",
        "            idxs += range(idx*self.batch_size, (idx+1)*self.batch_size)\n",
        "        datasets['train'] = Dataset.from_dict(datasets['train'][[idx for idx in idxs if idx < num_train]])\n",
        "\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def load_llm_preds(self, split):\n",
        "        labels = list()\n",
        "        rationales = list()\n",
        "        for idx in getattr(self, f'{split}_batch_idxs'):\n",
        "            with open(f'{self.data_root}/{self.dataset_name}/llm/{split}_CoT_{idx}.json') as f:\n",
        "                outputs = json.load(f)\n",
        "\n",
        "            for output in outputs:\n",
        "                rationale, label = self._parse_llm_output(output)\n",
        "\n",
        "                rationales.append(rationale)\n",
        "                labels.append(label)\n",
        "\n",
        "        return rationales, labels\n",
        "\n",
        "\n",
        "    def load_gpt_preds(self, split):\n",
        "        labels = list()\n",
        "        rationales = list()\n",
        "\n",
        "        with open(f'{self.data_root}/gpt-neox/{self.dataset_name}/{split}.json') as f:\n",
        "            outputs = json.load(f)\n",
        "\n",
        "        for output in outputs:\n",
        "            rationale, label = self._parse_gpt_output(output)\n",
        "\n",
        "            rationales.append(rationale)\n",
        "            labels.append(label)\n",
        "\n",
        "        return rationales, labels\n",
        "\n",
        "\n",
        "    def _post_process(self, datasets):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def _parse_llm_output(self, output):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def _parse_gpt_output(self, output):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class CQADatasetLoader(DatasetLoader):\n",
        "    def __init__(self):\n",
        "        dataset_name = 'cqa'\n",
        "        source_dataset_name = 'cos_e'\n",
        "        dataset_version = 'v1.11'\n",
        "        has_valid = False\n",
        "        split_map = {\n",
        "            'train': 'train',\n",
        "            'test': 'validation',\n",
        "        }\n",
        "        batch_size = 1000\n",
        "        train_batch_idxs = range(10)\n",
        "        test_batch_idxs = range(2)\n",
        "\n",
        "        super().__init__(dataset_name, source_dataset_name, dataset_version, has_valid, split_map,\n",
        "                 batch_size, train_batch_idxs, test_batch_idxs, valid_batch_idxs=None)\n",
        "\n",
        "\n",
        "    def _post_process(self, datasets):\n",
        "\n",
        "        def prepare_input(example):\n",
        "            question = example['question']\n",
        "            c_0 = example['choices'][0]\n",
        "            c_1 = example['choices'][1]\n",
        "            c_2 = example['choices'][2]\n",
        "            c_3 = example['choices'][3]\n",
        "            c_4 = example['choices'][4]\n",
        "\n",
        "            input = f'{question}\\nAnswer Choices:\\n(a) {c_0}\\n(b) {c_1}\\n(c) {c_2}\\n(d) {c_3}\\n(e) {c_4}'\n",
        "\n",
        "            example['input'] = input\n",
        "            example['label'] = example['answer']\n",
        "\n",
        "            return example\n",
        "\n",
        "        datasets = datasets.map(prepare_input)\n",
        "        datasets = datasets.remove_columns(['id', 'question', 'choices', 'answer', 'abstractive_explanation', 'extractive_explanation'])\n",
        "\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def _parse_llm_output(self, output):\n",
        "        rationale_label = output.split('Q:')[0]\n",
        "        rationale_label = rationale_label.rstrip()\n",
        "        rationale, label = rationale_label.split('So the answer is')\n",
        "        rationale = rationale.rstrip()\n",
        "\n",
        "        try:\n",
        "            label = re.split(r'\\(.\\)', label)[1].strip()\n",
        "            label = label if label[-1]!='.' else label[:-1]\n",
        "        except:\n",
        "            label = ' '\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "\n",
        "    def _parse_gpt_output(self, output):\n",
        "        rationale_label = output.split('Q:')[0]\n",
        "        rationale_label = rationale_label.rstrip().lstrip()\n",
        "        try:\n",
        "            rationale, label = rationale_label.split('So the answer is')\n",
        "            rationale = rationale.rstrip()\n",
        "        except:\n",
        "            rationale = ' '\n",
        "            label = ' '\n",
        "            return rationale, label\n",
        "\n",
        "        try:\n",
        "            label = re.split(r'\\(.\\)', label)[1].strip()\n",
        "            label = label if label[-1]!='.' else label[:-1]\n",
        "        except:\n",
        "            label = ' '\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "\n",
        "class SVAMPDatasetLoader(DatasetLoader):\n",
        "    def __init__(self):\n",
        "        dataset_name = 'svamp'\n",
        "        source_dataset_name = 'svamp'\n",
        "        dataset_version = None\n",
        "        has_valid = False\n",
        "        split_map = {\n",
        "            'train': 'train',\n",
        "            'test': 'test',\n",
        "        }\n",
        "        batch_size = 500\n",
        "        train_batch_idxs = range(2)\n",
        "        test_batch_idxs = range(1)\n",
        "\n",
        "\n",
        "        super().__init__(dataset_name, source_dataset_name, dataset_version, has_valid, split_map,\n",
        "                 batch_size, train_batch_idxs, test_batch_idxs, valid_batch_idxs=None)\n",
        "\n",
        "\n",
        "    def load_from_source(self):\n",
        "        with open(f'{self.data_root}/{self.dataset_name}/SVAMP.json') as f:\n",
        "            original_dataset = json.load(f)\n",
        "\n",
        "        dataset = list()\n",
        "        for data in original_dataset:\n",
        "            input = f'{data[\"Body\"]}\\n{data[\"Question\"]}'\n",
        "            equation = data[\"Equation\"]\n",
        "\n",
        "            dataset.append({\n",
        "                'input': input,\n",
        "                'label': equation,\n",
        "            })\n",
        "\n",
        "        idxs = np.random.RandomState(seed=0).permutation(len(dataset))\n",
        "        train_idxs = idxs[:800]\n",
        "        test_idxs = idxs[800:]\n",
        "\n",
        "        train_dataset = Dataset.from_list(np.array(dataset)[train_idxs].tolist())\n",
        "        test_dataset = Dataset.from_list(np.array(dataset)[test_idxs].tolist())\n",
        "\n",
        "        datasets = DatasetDict({\n",
        "            'train': train_dataset,\n",
        "            'test': test_dataset\n",
        "        })\n",
        "\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def _post_process(self, datasets):\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def _parse_llm_output(self, output):\n",
        "        rationale_label = output.split('Q:')[0]\n",
        "        rationale_label = rationale_label.rstrip()\n",
        "        try:\n",
        "            rationale, label = rationale_label.split('The answer is')\n",
        "        except:\n",
        "            rationale = ' '\n",
        "            label = ' '\n",
        "            return rationale, label\n",
        "\n",
        "        rationale = rationale.rstrip()\n",
        "        try:\n",
        "            label = re.search(r'\\(.*\\)', label).group(0)\n",
        "        except:\n",
        "            label = ' '\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "    def _parse_gpt_output(self, output):\n",
        "        rationale_label = output.split('Q:')[0]\n",
        "        rationale_label = rationale_label.rstrip().lstrip()\n",
        "        try:\n",
        "            rationale, label = rationale_label.split('The answer is')\n",
        "        except:\n",
        "            rationale = ' '\n",
        "            label = ' '\n",
        "            return rationale, label\n",
        "\n",
        "        rationale = rationale.rstrip()\n",
        "        try:\n",
        "            label = re.search(r'\\(.*\\)', label).group(0)\n",
        "        except:\n",
        "            label = ' '\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "\n",
        "class ASDivDatasetLoader(DatasetLoader):\n",
        "    def __init__(self):\n",
        "        dataset_name = 'asdiv'\n",
        "        dataset_version = None\n",
        "        has_valid = False\n",
        "        split_map = {\n",
        "            'train': 'train',\n",
        "            'test': 'test',\n",
        "        }\n",
        "        batch_size = 1000\n",
        "        train_batch_idxs = range(3)\n",
        "        test_batch_idxs = range(1)\n",
        "\n",
        "        super().__init__(dataset_name, dataset_version, has_valid, split_map,\n",
        "                 batch_size, train_batch_idxs, test_batch_idxs, valid_batch_idxs=None)\n",
        "\n",
        "\n",
        "    def load_from_source(self):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "    def _post_process(self, datasets):\n",
        "\n",
        "        def prepare_input(example):\n",
        "            example['input'] = example['Body'] + '\\n' + example['Question']\n",
        "            answer = example['Answer'].split(' ')[0]\n",
        "            example['label'] = answer\n",
        "\n",
        "            return example\n",
        "\n",
        "        datasets = datasets.map(prepare_input)\n",
        "        datasets = datasets.remove_columns(['Body', 'Question', 'Formula', 'Answer'])\n",
        "\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def _parse_llm_output(self, output):\n",
        "        rationale_label = output.split('Q:')[0]\n",
        "        rationale_label = rationale_label.rstrip()\n",
        "        try:\n",
        "            rationale, label = rationale_label.split('The answer is')\n",
        "        except:\n",
        "            rationale = ' '\n",
        "            label = ' '\n",
        "            return rationale, label\n",
        "\n",
        "        rationale = rationale.rstrip()\n",
        "        try:\n",
        "            label = re.search(r'\\(.*\\)', label).group(0)\n",
        "        except:\n",
        "            label = ' '\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "\n",
        "    def _parse_gpt_output(self, output):\n",
        "        raise NotImplementedError\n",
        "\n",
        "\n",
        "class ESNLIDatasetLoader(DatasetLoader):\n",
        "    def __init__(self, subset='full'):\n",
        "        dataset_name = 'esnli'\n",
        "        source_dataset_name = 'esnli'\n",
        "        dataset_version = None\n",
        "        has_valid = True\n",
        "        split_map = {\n",
        "            'train': 'train',\n",
        "            'valid': 'validation',\n",
        "            'test': 'test',\n",
        "        }\n",
        "        batch_size = 5500\n",
        "        if subset == 'full':\n",
        "            train_batch_idxs = range(100)\n",
        "        elif subset == 'small':\n",
        "            train_batch_idxs = range(10)\n",
        "        else:\n",
        "            raise ValueError\n",
        "        test_batch_idxs = range(2)\n",
        "        valid_batch_idxs = range(2)\n",
        "\n",
        "        super().__init__(dataset_name, source_dataset_name, dataset_version, has_valid, split_map,\n",
        "                 batch_size, train_batch_idxs, test_batch_idxs, valid_batch_idxs=valid_batch_idxs)\n",
        "\n",
        "\n",
        "    def _post_process(self, datasets):\n",
        "\n",
        "        def prepare_input(example):\n",
        "            if example['label'] == 0:\n",
        "                example['label'] = 'entailment'\n",
        "            elif example['label'] == 1:\n",
        "                example['label'] = 'neutral'\n",
        "            elif example['label'] == 2:\n",
        "                example['label'] = 'contradiction'\n",
        "\n",
        "            return example\n",
        "\n",
        "        datasets = datasets.map(prepare_input)\n",
        "        datasets = datasets.remove_columns(['explanation_1', 'explanation_2', 'explanation_3'])\n",
        "\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def _parse_llm_output(self, output):\n",
        "        rationale = output.split(\"Answer:\")[0].rstrip()\n",
        "        try:\n",
        "            label = output.split(\"Answer: \")[1].split(\"Premise\")[0].rstrip()\n",
        "        except:\n",
        "            label = ' '\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "\n",
        "    def _parse_gpt_output(self, output):\n",
        "        rationale = output.split(\"Answer:\")[0].rstrip().lstrip()\n",
        "        try:\n",
        "            label = output.split(\"Answer: \")[1].split(\"Premise\")[0].rstrip()\n",
        "        except:\n",
        "            label = ' '\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "\n",
        "class ANLIDatasetLoader(DatasetLoader):\n",
        "    def __init__(self, dataset_name, source_dataset_name, dataset_version, has_valid, split_map,\n",
        "                 batch_size, train_batch_idxs, test_batch_idxs, valid_batch_idxs):\n",
        "\n",
        "        super().__init__(dataset_name, source_dataset_name, dataset_version, has_valid, split_map,\n",
        "                 batch_size, train_batch_idxs, test_batch_idxs, valid_batch_idxs=valid_batch_idxs)\n",
        "\n",
        "    def _post_process(self, datasets):\n",
        "\n",
        "        def label_idx2text(example):\n",
        "            if example['label'] == 0:\n",
        "                example['label'] = 'entailment'\n",
        "            elif example['label'] == 1:\n",
        "                example['label'] = 'neutral'\n",
        "            elif example['label'] == 2:\n",
        "                example['label'] = 'contradiction'\n",
        "            return example\n",
        "\n",
        "        datasets = datasets.map(label_idx2text)\n",
        "        datasets = datasets.remove_columns(['uid', 'reason'])\n",
        "\n",
        "        return datasets\n",
        "\n",
        "\n",
        "    def _parse_llm_output(self, output):\n",
        "        try:\n",
        "            rationale, label = output.split(\"Premise:\")[0].rstrip().split(\"So the answer is\")\n",
        "        except:\n",
        "            rationale = ''\n",
        "            label = ''\n",
        "\n",
        "        rationale = rationale.rstrip()\n",
        "        label = label.lstrip()[:-1]\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "\n",
        "    def _parse_gpt_output(self, output):\n",
        "        try:\n",
        "            rationale, label = output.split(\"Premise:\")[0].rstrip().lstrip().split(\"So the answer is\")\n",
        "        except:\n",
        "            try:\n",
        "                rationale, label = output.split(\"Premise:\")[0].rstrip().lstrip().split(\"The answer is\")\n",
        "            except:\n",
        "                rationale = ''\n",
        "                label = ''\n",
        "\n",
        "\n",
        "        rationale = rationale.rstrip()\n",
        "        label = label.lstrip()[:-1]\n",
        "\n",
        "        return rationale, label\n",
        "\n",
        "\n",
        "class ANLI1DatasetLoader(ANLIDatasetLoader):\n",
        "    def __init__(self):\n",
        "        dataset_name = 'anli1'\n",
        "        source_dataset_name = 'anli'\n",
        "        dataset_version = None\n",
        "        has_valid = True\n",
        "        split_map = {\n",
        "            'train': 'train_r1',\n",
        "            'valid': 'dev_r1',\n",
        "            'test': 'test_r1',\n",
        "        }\n",
        "        batch_size = 5000\n",
        "        train_batch_idxs = range(4)\n",
        "        test_batch_idxs = range(1)\n",
        "        valid_batch_idxs = range(1)\n",
        "\n",
        "        super().__init__(dataset_name, source_dataset_name, dataset_version, has_valid, split_map,\n",
        "                 batch_size, train_batch_idxs, test_batch_idxs, valid_batch_idxs=valid_batch_idxs)"
      ],
      "metadata": {
        "id": "WrfdN6Zngp8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Metrics"
      ],
      "metadata": {
        "id": "0HjgRJ9chvQm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import accelerate\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "def compute_text_acc(preds, labels):\n",
        "    return np.mean(np.array(preds) == np.array(labels))\n",
        "\n",
        "def compute_text_loss(preds, labels):\n",
        "    le = LabelEncoder()\n",
        "    le.fit(np.array(labels))\n",
        "    le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    labels = np.array([le_dict.get(each, -1) for each in labels])\n",
        "    preds = np.array([le_dict.get(each, -1) for each in preds])\n",
        "    return mean_squared_error(labels, preds)\n",
        "\n",
        "def compute_equation_acc(preds, labels):\n",
        "    preds = [eval_equation(pred) for pred in preds]\n",
        "    labels = [eval_equation(label) for label in labels]\n",
        "    return np.mean(np.array(preds) == np.array(labels))\n",
        "\n",
        "def compute_equation_loss(preds, labels):\n",
        "    preds = [eval_equation(pred) for pred in preds]\n",
        "    labels = [eval_equation(label) for label in labels]\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    le.fit(np.array(labels))\n",
        "    le_dict = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "    labels = np.array([le_dict.get(each, -1) for each in labels])\n",
        "    preds = np.array([le_dict.get(each, -1) for each in preds])\n",
        "    return mean_squared_error(labels, preds)\n",
        "\n",
        "def eval_equation(equation):\n",
        "    try:\n",
        "        answer = eval(equation)\n",
        "    except:\n",
        "        answer = np.nan\n",
        "\n",
        "    return answer\n",
        "\n",
        "\n",
        "def compute_metrics_text(tokenizer):\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        decoded_preds = tokenizer.batch_decode(predictions[0], skip_special_tokens=True)\n",
        "\n",
        "        labels = np.where(labels[0] != -100, labels[0], tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        acc = np.mean(np.array(decoded_preds) == np.array(decoded_labels))\n",
        "\n",
        "        return {'accuracy': acc}\n",
        "\n",
        "    return compute_metrics\n",
        "\n",
        "\n",
        "def compute_metrics_text_aux(tokenizer):\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        acc = np.mean(np.array(decoded_preds) == np.array(decoded_labels))\n",
        "\n",
        "        return {'accuracy': acc}\n",
        "\n",
        "    return compute_metrics\n",
        "\n",
        "\n",
        "\n",
        "def compute_metrics_equation(tokenizer):\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        decoded_preds = tokenizer.batch_decode(predictions[0], skip_special_tokens=True)\n",
        "\n",
        "        labels = np.where(labels[0] != -100, labels[0], tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        preds = list()\n",
        "        for pred in decoded_preds:\n",
        "            preds.append(eval_equation(pred))\n",
        "\n",
        "        labels = list()\n",
        "        for label in decoded_labels:\n",
        "            labels.append(eval_equation(label))\n",
        "\n",
        "        acc = np.mean(np.array(preds) == np.array(labels))\n",
        "\n",
        "        return {'accuracy': acc}\n",
        "\n",
        "    return compute_metrics\n",
        "\n",
        "\n",
        "def compute_metrics_equation_aux(tokenizer):\n",
        "    def compute_metrics(eval_pred):\n",
        "        predictions, labels = eval_pred\n",
        "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "        preds = list()\n",
        "        for pred in decoded_preds:\n",
        "            preds.append(eval_equation(pred))\n",
        "\n",
        "        labels = list()\n",
        "        for label in decoded_labels:\n",
        "            labels.append(eval_equation(label))\n",
        "\n",
        "        acc = np.mean(np.array(preds) == np.array(labels))\n",
        "\n",
        "        return {'accuracy': acc}\n",
        "\n",
        "    return compute_metrics"
      ],
      "metadata": {
        "id": "J_lfwSqzhGJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Utils"
      ],
      "metadata": {
        "id": "ddvMr8d2h3tj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from typing import Any, Dict, List, Optional, Tuple, Union\n",
        "from torch import nn\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers import Seq2SeqTrainer\n",
        "import accelerate\n",
        "\n",
        "\"\"\"T5 Multi-Task by Task Prefix\n",
        "\"\"\"\n",
        "class TaskPrefixDataCollator(DataCollatorForSeq2Seq):\n",
        "    def __call__(self, features, return_tensors=None):\n",
        "        features_df = pd.DataFrame(features)\n",
        "        pred_features = features_df.loc[:, ~features_df.columns.isin(['aux_labels', 'expl_input_ids', 'expl_attention_mask'])].to_dict('records')\n",
        "        expl_features = features_df.loc[:, ~features_df.columns.isin(['labels', 'input_ids', 'attention_mask'])].rename(\n",
        "            columns={'aux_labels': 'labels', 'expl_input_ids': 'input_ids', 'expl_attention_mask': 'attention_mask'}).to_dict('records')\n",
        "\n",
        "        pred_features = super().__call__(pred_features, return_tensors)\n",
        "        expl_features = super().__call__(expl_features, return_tensors)\n",
        "\n",
        "        return {\n",
        "            'pred': pred_features,\n",
        "            'expl': expl_features,\n",
        "        }\n",
        "\n",
        "\n",
        "class TaskPrefixTrainer(Seq2SeqTrainer):\n",
        "    def __init__(self, alpha, output_rationale, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.alpha = alpha\n",
        "        self.output_rationale = output_rationale\n",
        "\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        pred_outputs = model(**inputs['pred'])\n",
        "        expl_outputs = model(**inputs['expl'])\n",
        "\n",
        "        loss = self.alpha * pred_outputs.loss + (1. - self.alpha) * expl_outputs.loss\n",
        "\n",
        "        return (loss, {'pred': pred_outputs, 'expl': expl_outputs}) if return_outputs else loss\n",
        "\n",
        "\n",
        "    def prediction_step(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        inputs: Dict[str, Union[torch.Tensor, Any]],\n",
        "        prediction_loss_only: bool,\n",
        "        ignore_keys: Optional[List[str]] = None\n",
        "    ) -> Tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n",
        "\n",
        "        pred_outputs = super().prediction_step(model, inputs['pred'], prediction_loss_only=False, ignore_keys=ignore_keys)\n",
        "        if self.output_rationale:\n",
        "            expl_outputs = super().prediction_step(model, inputs['expl'], prediction_loss_only=False, ignore_keys=ignore_keys)\n",
        "        else:\n",
        "            expl_outputs = pred_outputs # placeholder only\n",
        "\n",
        "        loss = self.alpha * pred_outputs[0]  + (1 - self.alpha) * expl_outputs[0]\n",
        "\n",
        "        return (\n",
        "            loss,\n",
        "            [pred_outputs[1], expl_outputs[1]],\n",
        "            [pred_outputs[2], expl_outputs[2]],\n",
        "        )"
      ],
      "metadata": {
        "id": "JL5qUeqTh5nH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Trainer Utils"
      ],
      "metadata": {
        "id": "bR8LKujNiFSx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "import logging\n",
        "\n",
        "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
        "from transformers import T5ForConditionalGeneration\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "from transformers.trainer_utils import set_seed\n",
        "import accelerate\n",
        "\n",
        "# from model_utils import TaskPrefixDataCollator, TaskPrefixTrainer\n",
        "\n",
        "\n",
        "def get_config_dir(args):\n",
        "    return f'{args[\"dataset\"]}/{args[\"from_pretrained\"].split(\"/\")[0]}/{args[\"model_type\"]}/{args[\"llm\"]}/{args[\"subsample\"]}/{args[\"label_type\"]}/{args[\"alpha\"]}/{args[\"max_input_length\"]}/{args[\"grad_steps\"]*args[\"batch_size\"]}/{args[\"optimizer_name\"]}/{args[\"lr\"]}'\n",
        "\n",
        "\n",
        "def train_and_evaluate(args, run, tokenizer, tokenized_datasets, compute_metrics):\n",
        "    set_seed(run)\n",
        "\n",
        "    model = T5ForConditionalGeneration.from_pretrained(args['from_pretrained'])\n",
        "\n",
        "    if args['parallelize']:\n",
        "        model.parallelize()\n",
        "\n",
        "    config_dir = get_config_dir(args)\n",
        "    output_dir = f'ckpts/{config_dir}/{run}'  # for model ckpts\n",
        "    logging_dir = f'logs/{config_dir}/{run}'  # for training logs\n",
        "\n",
        "    if args['no_log']:\n",
        "        logging_strategy = 'no'\n",
        "        logging_dir = None\n",
        "    else:\n",
        "        logging_strategy = 'steps'\n",
        "\n",
        "    # clear output dir if already exists\n",
        "    if os.path.exists(output_dir):\n",
        "        logging.info('Found existing ckpt directory. Deleted the old directory for the latest run.')\n",
        "        shutil.rmtree(output_dir)\n",
        "\n",
        "    training_args = Seq2SeqTrainingArguments(\n",
        "        output_dir,\n",
        "        remove_unused_columns = False,\n",
        "        evaluation_strategy = 'steps',\n",
        "        eval_steps=args['eval_steps'],\n",
        "        save_strategy='no',\n",
        "        save_steps=args['eval_steps'],\n",
        "        logging_dir=logging_dir,\n",
        "        logging_strategy=logging_strategy,\n",
        "        logging_steps=args['eval_steps'],\n",
        "        max_steps=args['max_steps'],\n",
        "        learning_rate=args['lr'],\n",
        "        gradient_accumulation_steps=args['grad_steps'],\n",
        "        per_device_train_batch_size=args['batch_size'],\n",
        "        per_device_eval_batch_size=args['batch_size'],\n",
        "        predict_with_generate=True,\n",
        "        seed=run,\n",
        "        local_rank=args['local_rank'],\n",
        "        bf16=args['bf16'],\n",
        "        generation_max_length=args['gen_max_len'],\n",
        "        prediction_loss_only=False,\n",
        "    )\n",
        "\n",
        "    if args['model_type'] == 'task_prefix':\n",
        "        data_collator = TaskPrefixDataCollator(tokenizer=tokenizer, model=model)\n",
        "    elif args['model_type'] == 'standard':\n",
        "        data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "\n",
        "    trainer_kwargs = {\n",
        "        'alpha': args['alpha'],\n",
        "        'output_rationale': args['output_rationale'],\n",
        "        'model': model,\n",
        "        'args': training_args,\n",
        "        'train_dataset': tokenized_datasets[\"train\"],\n",
        "        'eval_dataset': {'test': tokenized_datasets[\"test\"],},\n",
        "        'data_collator': data_collator,\n",
        "        'tokenizer': tokenizer,\n",
        "        'compute_metrics': compute_metrics,\n",
        "    }\n",
        "\n",
        "\n",
        "    if args['model_type'] == 'task_prefix':\n",
        "        trainer = TaskPrefixTrainer(**trainer_kwargs)\n",
        "    elif args['model_type'] == 'standard':\n",
        "        trainer_kwargs['pop']('alpha')\n",
        "        trainer_kwargs['pop']('output_rationale')\n",
        "        trainer = Seq2SeqTrainer(**trainer_kwargs)\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "tOD_H0-BiGxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RUN"
      ],
      "metadata": {
        "id": "wihMWGIcilmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "\n",
        "from datasets import DatasetDict, concatenate_datasets\n",
        "from transformers import AutoTokenizer\n",
        "import accelerate\n",
        "\n",
        "# from data_utils import CQADatasetLoader, SVAMPDatasetLoader, ESNLIDatasetLoader, ANLI1DatasetLoader, ASDivDatasetLoader\n",
        "# from metrics import compute_text_acc, compute_equation_acc, compute_metrics_text, compute_metrics_equation, compute_metrics_text_aux, compute_metrics_equation_aux\n",
        "# from train_utils import train_and_evaluate\n",
        "\n",
        "\n",
        "def run(args):\n",
        "    #### Prepare datasets\n",
        "    if args['dataset'] == 'cqa':\n",
        "        dataset_loader = CQADatasetLoader()\n",
        "    elif args['dataset'] == 'svamp':\n",
        "        dataset_loader = SVAMPDatasetLoader()\n",
        "    elif args['dataset'] == 'esnli':\n",
        "        dataset_loader = ESNLIDatasetLoader()\n",
        "    elif args['dataset'] == 'anli1':\n",
        "        dataset_loader = ANLI1DatasetLoader()\n",
        "    elif args['dataset'] == 'asdiv':  # NOTE: for augmenting SVAMP only\n",
        "        dataset_loader = SVAMPDatasetLoader()\n",
        "        dataset_loader_svamp = SVAMPDatasetLoader()\n",
        "        dataset_loader_asdiv = ASDivDatasetLoader()\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    if args['dataset'] == 'asdiv':\n",
        "        datasets_svamp = dataset_loader_svamp.load_from_json()\n",
        "        datasets_asdiv = dataset_loader_asdiv.load_from_json()\n",
        "        datasets = DatasetDict({\n",
        "            'train': concatenate_datasets([datasets_svamp['train'], datasets_asdiv['train']]),\n",
        "            'test': datasets_svamp['test']\n",
        "        })\n",
        "    else:\n",
        "        datasets = dataset_loader.load_from_json()\n",
        "\n",
        "    if args['llm'] is None:\n",
        "        pass\n",
        "    elif args['llm'] == 'palm':\n",
        "        if args['dataset'] == 'asdiv':\n",
        "            # training set = SVAMP training + ASDiv training\n",
        "            train_llm_rationales_svamp, train_llm_labels_svamp = dataset_loader_svamp.load_llm_preds(split='train')\n",
        "            train_llm_rationales_asdiv, train_llm_labels_asdiv = dataset_loader_asdiv.load_llm_preds(split='train')\n",
        "            train_llm_rationales = train_llm_rationales_svamp + train_llm_rationales_asdiv\n",
        "            train_llm_labels = train_llm_labels_svamp + train_llm_labels_asdiv\n",
        "            # test set = SVAMP test\n",
        "            test_llm_rationales, test_llm_labels = dataset_loader_svamp.load_llm_preds(split='test')\n",
        "        else:\n",
        "            train_llm_rationales, train_llm_labels = dataset_loader.load_llm_preds(split='train')\n",
        "            test_llm_rationales, test_llm_labels = dataset_loader.load_llm_preds(split='test')\n",
        "    elif args['llm'] == 'gpt':\n",
        "        train_llm_rationales, train_llm_labels = dataset_loader.load_gpt_preds(split='train')\n",
        "        test_llm_rationales, test_llm_labels = dataset_loader.load_gpt_preds(split='test')\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    if args['llm'] is not None:\n",
        "        datasets['train'] = datasets['train'].add_column('llm_label', train_llm_labels)\n",
        "        datasets['test'] = datasets['test'].add_column('llm_label', test_llm_labels)\n",
        "        datasets['train'] = datasets['train'].add_column('llm_rationale', train_llm_rationales)\n",
        "        datasets['test'] = datasets['test'].add_column('llm_rationale', test_llm_rationales)\n",
        "\n",
        "    if args['subsample'] < 1.0:\n",
        "        datasets['train'] = datasets['train'].train_test_split(test_size=1.0-args['subsample'], seed=args['run'])['train']\n",
        "\n",
        "    if dataset_loader.has_valid:\n",
        "        if args['llm'] is None:\n",
        "            pass\n",
        "        elif args['llm'] == 'palm':\n",
        "            valid_llm_rationales, valid_llm_labels = dataset_loader.load_llm_preds(split='valid')\n",
        "        elif args['llm'] == 'gpt':\n",
        "            valid_llm_rationales, valid_llm_labels = dataset_loader.load_gpt_preds(split='valid')\n",
        "        else:\n",
        "            raise ValueError\n",
        "\n",
        "        datasets['valid'] = datasets['valid'].add_column('llm_label', valid_llm_labels)\n",
        "        datasets['valid'] = datasets['valid'].add_column('llm_rationale', valid_llm_rationales)\n",
        "    else:\n",
        "        train_valid_datasets = datasets['train'].train_test_split(test_size=0.1, seed=0)\n",
        "\n",
        "        datasets = DatasetDict({\n",
        "            'train': train_valid_datasets['train'],\n",
        "            'valid': train_valid_datasets['test'],\n",
        "            'test': datasets['test'],\n",
        "        })\n",
        "\n",
        "    if args['label_type'] == 'gt':\n",
        "        pass\n",
        "    elif args['label_type'] == 'llm' and args['llm'] is not None:\n",
        "        if args['dataset'] not in ['svamp', 'asdiv']:\n",
        "            train_label_acc = compute_text_acc(datasets['train']['llm_label'], datasets['train']['label'])\n",
        "            test_label_acc = compute_text_acc(datasets['test']['llm_label'], datasets['test']['label'])\n",
        "            train_label_loss = compute_text_loss(datasets['train']['llm_label'], datasets['train']['label'])\n",
        "            test_label_loss = compute_text_loss(datasets['test']['llm_label'], datasets['test']['label'])\n",
        "\n",
        "        else:\n",
        "            train_label_acc = compute_equation_acc(datasets['train']['llm_label'], datasets['train']['label'])\n",
        "            test_label_acc = compute_equation_acc(datasets['test']['llm_label'], datasets['test']['label'])\n",
        "            train_label_loss = compute_equation_loss(datasets['train']['llm_label'], datasets['train']['label'])\n",
        "            test_label_loss = compute_equation_loss(datasets['test']['llm_label'], datasets['test']['label'])\n",
        "\n",
        "        print(f'LLM Train Acc: {train_label_acc:.4f}')\n",
        "        print(f'LLM Test Acc: {test_label_acc:.4f}')\n",
        "        print(f'LLM Train Loss: {train_label_loss:.4f}')\n",
        "        print(f'LLM Test Loss: {test_label_loss:.4f}')\n",
        "\n",
        "        datasets['train'] = datasets['train'].remove_columns('label')\n",
        "        datasets['train'] = datasets['train'].add_column('label', datasets['train']['llm_label'])\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "    if args['llm'] is not None:\n",
        "        if 'rationale' in datasets['train'].column_names:\n",
        "            datasets = datasets.remove_columns('rationale')\n",
        "        datasets = datasets.rename_column('llm_rationale', 'rationale')\n",
        "\n",
        "\n",
        "    #### Prepare datasets Prepare data for training\n",
        "    tokenizer = AutoTokenizer.from_pretrained(args['from_pretrained'], use_fast=False)\n",
        "\n",
        "    if 'nli' in args['dataset']:\n",
        "        datasets = datasets.map(\n",
        "            lambda example: {'input': tokenizer.eos_token.join([example['premise'], example['hypothesis']])},\n",
        "            remove_columns=['premise', 'hypothesis'],\n",
        "        )\n",
        "\n",
        "\n",
        "    if args['model_type'] == 'task_prefix' and args['llm'] is not None:\n",
        "        def tokenize_function(examples):\n",
        "            model_inputs = tokenizer(['predict: ' + text for text in examples['input']], max_length=args['max_input_length'], truncation=True)\n",
        "            expl_model_inputs = tokenizer(['explain: ' + text for text in examples['input']], max_length=args['max_input_length'], truncation=True)\n",
        "            model_inputs['expl_input_ids'] = expl_model_inputs['input_ids']\n",
        "            model_inputs['expl_attention_mask'] = expl_model_inputs['attention_mask']\n",
        "\n",
        "            with tokenizer.as_target_tokenizer():\n",
        "                label_output_encodings = tokenizer(examples['label'], max_length=256, truncation=True)\n",
        "                rationale_output_encodings = tokenizer(examples['rationale'], max_length=256, truncation=True)\n",
        "\n",
        "            model_inputs['labels'] = label_output_encodings['input_ids']\n",
        "            model_inputs['aux_labels'] = rationale_output_encodings['input_ids']\n",
        "\n",
        "            return model_inputs\n",
        "\n",
        "    elif args['model_type'] == 'standard':\n",
        "        def tokenize_function(examples):\n",
        "            model_inputs = tokenizer(\n",
        "                examples['input'],\n",
        "                max_length=args['max_input_length'],\n",
        "                truncation=True\n",
        "            )\n",
        "\n",
        "            with tokenizer.as_target_tokenizer():\n",
        "                label_output_encodings = tokenizer(examples['label'], max_length=256, truncation=True)\n",
        "\n",
        "            model_inputs['labels'] = label_output_encodings['input_ids']\n",
        "\n",
        "            return model_inputs\n",
        "\n",
        "    else:\n",
        "        raise ValueError\n",
        "\n",
        "\n",
        "    if args['llm'] is None:\n",
        "        tokenized_datasets = datasets.map(\n",
        "            tokenize_function,\n",
        "            remove_columns=['input', 'label'],\n",
        "            batched=True\n",
        "        )\n",
        "    else:\n",
        "        tokenized_datasets = datasets.map(\n",
        "            tokenize_function,\n",
        "            remove_columns=['input', 'rationale', 'label', 'llm_label'],\n",
        "            batched=True\n",
        "        )\n",
        "\n",
        "\n",
        "    if args['model_type'] == 'standard':\n",
        "        if args['dataset'] not in ['svamp', 'asdiv']:\n",
        "            compute_metrics = compute_metrics_text_aux(tokenizer)\n",
        "        else:\n",
        "            compute_metrics = compute_metrics_equation_aux(tokenizer)\n",
        "\n",
        "    else:\n",
        "        if args['dataset'] not in ['svamp', 'asdiv']:\n",
        "            compute_metrics = compute_metrics_text(tokenizer)\n",
        "        else:\n",
        "            compute_metrics = compute_metrics_equation(tokenizer)\n",
        "\n",
        "\n",
        "    train_and_evaluate(args, args['run'], tokenizer, tokenized_datasets, compute_metrics)"
      ],
      "metadata": {
        "id": "mShOW332Ywvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Training"
      ],
      "metadata": {
        "id": "g161kMbzy6hA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'dataset': 'anli1',\n",
        "    'subsample': 1.0,\n",
        "    'alpha': 0.5,\n",
        "    'max_steps': 5_000,\n",
        "    'eval_steps': 500,\n",
        "    'batch_size': 1,\n",
        "    'optimizer_name': 'AdamW',\n",
        "    'lr': 5e-5,\n",
        "    'run': 0,\n",
        "    'from_pretrained': 't5-small',\n",
        "    'label_type': 'gt',\n",
        "    'llm': 'palm',\n",
        "    'max_input_length': 2048,\n",
        "    'grad_steps': 1,\n",
        "    'local_rank': -1,\n",
        "    'gen_max_len': 64,\n",
        "    'parallelize': True,\n",
        "    'model_type': 'standard',\n",
        "    'bf16': False,\n",
        "    'no_log': False,\n",
        "    'output_rationale': True,\n",
        "    }\n",
        "\n",
        "run(args)"
      ],
      "metadata": {
        "id": "Q2lu2UNctkfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Standard Knowledge Distillation Training"
      ],
      "metadata": {
        "id": "I2NRY-BEy_5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'dataset': 'anli1',\n",
        "    'subsample': 1.0,\n",
        "    'alpha': 0.5,\n",
        "    'max_steps': 5_000,\n",
        "    'eval_steps': 500,\n",
        "    'batch_size': 1,\n",
        "    'optimizer_name': 'AdamW',\n",
        "    'lr': 5e-5,\n",
        "    'run': 0,\n",
        "    'from_pretrained': 't5-small',\n",
        "    'label_type': 'llm',\n",
        "    'llm': 'palm',\n",
        "    'max_input_length': 2048,\n",
        "    'grad_steps': 1,\n",
        "    'local_rank': -1,\n",
        "    'gen_max_len': 64,\n",
        "    'parallelize': True,\n",
        "    'model_type': 'standard',\n",
        "    'bf16': False,\n",
        "    'no_log': False,\n",
        "    'output_rationale': True,\n",
        "    }\n",
        "\n",
        "run(args)"
      ],
      "metadata": {
        "id": "bBbojWqHy_5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-by-Step Distillation Training"
      ],
      "metadata": {
        "id": "PXHh6xK8zAFu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = {\n",
        "    'dataset': 'anli1',\n",
        "    'subsample': 1.0,\n",
        "    'alpha': 0.5,\n",
        "    'max_steps': 5_000,\n",
        "    'eval_steps': 500,\n",
        "    'batch_size': 1,\n",
        "    'optimizer_name': 'AdamW',\n",
        "    'lr': 5e-5,\n",
        "    'run': 0,\n",
        "    'from_pretrained': 't5-small',\n",
        "    'label_type': 'llm',\n",
        "    'llm': 'palm',\n",
        "    'max_input_length': 2048,\n",
        "    'grad_steps': 1,\n",
        "    'local_rank': -1,\n",
        "    'gen_max_len': 64,\n",
        "    'parallelize': True,\n",
        "    'model_type': 'task_prefix',\n",
        "    'bf16': False,\n",
        "    'no_log': False,\n",
        "    'output_rationale': True,\n",
        "    }\n",
        "\n",
        "run(args)"
      ],
      "metadata": {
        "id": "japzsMQDzAFv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}